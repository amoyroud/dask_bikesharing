{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model\n",
    "\n",
    "This notebook serves to perform a deeper analysis of the best model out of all the ones that were tested in the process of building a model to predict the count of riders on an hourly basis.\n",
    "\n",
    "In the cell below, we reconstruct that model and store it so we can evaluate its performance and explore the predictions it makes against the actual values. On an initial exploration of that particular model and the predictions that it gave, we noticed that it actually will make negative predictions from time to time. This clearly is not good behavior since in the end the value that the model aims to predict is a count. To combat this issue, we adjust the model's predictions by simply replacing negative values with 0. This resulted in a slight increase in $R^{2}$ from 0.8638 to 0.864, but most importantly ensured that the model will never return invalid values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-28e74d4edaeb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# Perform all the steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mwith_created\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[0mdummified\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_created\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mwith_selected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdummified\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\SUBMIT-20190511T134258Z-001\\DASK Assignment\\utils.py\u001b[0m in \u001b[0;36mupdate_df\u001b[1;34m(data, ops)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAn\u001b[0m \u001b[0miterable\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0ma\u001b[0m \u001b[0msignature\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mDataFrame\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mall\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mops\u001b[0m \u001b[0mapplied\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \"\"\"\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mop\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import featuretools\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from preprocessing import *\n",
    "from feature_creation import *\n",
    "from utils import PATH, SEED, update_df, get_train, get_holdout\n",
    "\n",
    "# We noticed that the regular GradientBoostingRegressor that was the final model selected was returning negative\n",
    "# predictions for the count on occasion (~50 predictions). We made a class to return the predictions as normal\n",
    "# but with negative predictions replaced with 0.\n",
    "\n",
    "class NonNegativeGBR(GradientBoostingRegressor):\n",
    "    def predict(self, X):\n",
    "        predictions = super(NonNegativeGBR, self).predict(X)\n",
    "        max_zero = da.vectorize(lambda x: max(0, x))\n",
    "        return max_zero(predictions)\n",
    "    \n",
    "# All the preprocessing steps\n",
    "pp = [drop_registered, drop_casual, drop_date, drop_instant, year_as_bool,\n",
    "      season_as_category, month_as_category, weekday_as_category,\n",
    "      hour_as_category, holiday_as_bool, working_day_as_bool, weather_sit_as_category,\n",
    "     change_season, change_weather_sit]\n",
    "\n",
    "# The feature creation steps that yielded the best results\n",
    "fc = [commute_hours, weather_cluster, deep_features]\n",
    "\n",
    "# The final features that had been selected by the Recursive Feature Elimination\n",
    "final_features = ['yr', 'workingday', 'temp', 'atemp', 'hum', 'windspeed', 'commute_hours', 'seasons.STD(bikeshare_hourly.hum)', 'seasons.MAX(bikeshare_hourly.temp)', 'seasons.MAX(bikeshare_hourly.windspeed)', 'seasons.MIN(bikeshare_hourly.temp)', 'seasons.MEAN(bikeshare_hourly.temp)', 'seasons.MEAN(bikeshare_hourly.hum)', 'seasons.MEAN(bikeshare_hourly.windspeed)', 'mnth_2', 'mnth_3', 'mnth_4', 'mnth_5', 'mnth_6', 'mnth_8', 'mnth_12', 'hr_0', 'hr_1', 'hr_2', 'hr_3', 'hr_4', 'hr_5', 'hr_6', 'hr_7', 'hr_8', 'hr_9', 'hr_10', 'hr_11', 'hr_12', 'hr_13', 'hr_14', 'hr_15', 'hr_16', 'hr_17', 'hr_18', 'hr_19', 'hr_20', 'hr_21', 'hr_22', 'hr_23', 'weekday_4', 'weekday_5', 'weathersit_Heavy Precipitation', 'weather_cluster_1', 'weather_cluster_2']\n",
    "data = dd.read_csv(PATH)\n",
    "target = data[\"cnt\"]\n",
    "\n",
    "    # Perform all the steps\n",
    "preprocessed = update_df(data, pp)\n",
    "with_created = update_df(preprocessed, fc)\n",
    "dummified = dd.get_dummies(with_created)\n",
    "with_selected = dummified.loc[:, final_features]\n",
    "\n",
    "# The model with the chosen parameters from the Grid Search Cross Validation\n",
    "model = NonNegativeGBR(n_estimators=750, learning_rate=0.1, random_state=SEED)\n",
    "\n",
    "# Fit the model\n",
    "model = model.fit(get_train(with_selected), get_train(target))\n",
    "\n",
    "# Retrieve the score\n",
    "score = model.score(get_holdout(with_selected), get_holdout(target))\n",
    "\n",
    "score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions vs. Actual Values over time\n",
    "\n",
    "Next, we make a plot of both the predicted and the actual values over time. We use one week for the example since more than this makes it difficult to see any patterns in the data. The rise and fall in the number of rides over time can clearly be seen in the chart below, and the predictions for these values also manages to keep in time with these fluctuations in the number of rides as hours progress. We were pleased to see that our model was able to capture so well the patterns that naturally exist in the number of rides taken per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The model's predictions on the holdout\n",
    "predictions = model.predict(get_holdout(with_selected))\n",
    "\n",
    "# The actual values\n",
    "pred_actual = pd.DataFrame({\"time_index\": list(range(len(predictions))),\n",
    "                            \"predicted\": predictions,\n",
    "                            \"actual\": get_holdout(target)})\n",
    "\n",
    "# Just look at one week (more than this is hard to see)\n",
    "by_time = pred_actual[:118] # one week\n",
    "\n",
    "# Convert data from tabular to transactional format\n",
    "melted = pd.melt(by_time, value_vars=[\"predicted\", \"actual\"], id_vars=[\"time_index\"])\n",
    "\n",
    "# Generate the graph\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "g = sns.scatterplot(data=melted, \n",
    "                    x=\"time_index\",\n",
    "                    y=\"value\",\n",
    "                    hue=\"variable\",\n",
    "                    palette={\"predicted\": \"#a12113\", \"actual\": \"#ff6352\"})\n",
    "plt.title(\"Predictions and Actual Values vs. Time (One Week Period)\")\n",
    "plt.xlabel(\"Time Index\")\n",
    "plt.ylabel(\"Count of Rides\")\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles[1:], labels=labels[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions vs. Actual Values\n",
    "\n",
    "Last, a scatterplot of predicted values against actual values. Of course, the ideal situation would be a perfect line which would mean that the prediction matches the actual value for all actual values. We don't get quite this, but you can still see the tight linear relationship between our predictions and the actual values. This is the chart that also led us to realize that our original model was making negative predictions. This time it can clearly be seen that the lowest value for our predictions is 0, which is exactly what should be the case when predicting a count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of predictions vs. Actual\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "sns.scatterplot(data=pred_actual, x=\"predicted\", y=\"actual\", color=\"firebrick\")\n",
    "plt.title(\"Predictions vs. Actual Values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double-checking the updated model\n",
    "\n",
    "Lastly, we just wanted to make sure that doing a simple replacement of negative values with zeroes was a valid choice and that it was working properly. First, we check that there are no negative values in the predictions and find none. Next, we wanted to see if there were any hours that were close to zero - seeing that the real minimum of the actual values is 1, we see that 0 is a reasonable prediction to make. Lastly, we check under what circumstances we make a prediction of 0 to see if it makes sense, and we see that it does. The majority of the examples are between midnight and 4 AM, and the exceptions appear to be exclusively on holidays which also logically makes sense. This helps us feel assured that our model is doing the right thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we don't have any more negative predictions\n",
    "np.min(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How far off could we be?\n",
    "np.min(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same check as before\n",
    "predictions[predictions < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of curiousity, what were the hours where we predicted a negative value or zero?\n",
    "get_holdout(data)[predictions==0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
