{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out Models\n",
    "\n",
    "In this file is several different executions of our pipeline with different combinations of feature creation methods, feature selection methods, models, and so on. You can kind of think of this file as a type of grid search cross validation where we're not testing only model parameters but different combinations of features as well.\n",
    "\n",
    "#### Warning: Do NOT try re-running this notebook, the whole thing will take several hours to run.\n",
    "\n",
    "If there is a particular pipeline you would like to retry, run them individually and before doing so be aware of how long it will take. Rhere are start and stop timestamps in the output of each run of a pipeline, so please consult those before beginning.\n",
    "\n",
    "The best model of all of these is the second-to-last, which is all the basic features plus deep feature synthesis on the season plus recursive feature elimination for selection. ($R^{2}$ of 0.8638)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outside modules\n",
    "import warnings; warnings.simplefilter('ignore') # suppress warnings\n",
    "import dask.dataframe as dd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Home-made modules\n",
    "from utils import pipeline_casero\n",
    "from preprocessing import *\n",
    "from feature_creation import *\n",
    "from feature_selection import  *\n",
    "from dim_reduction import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dd.read_csv(\"https://gist.githubusercontent.com/catyselman/9353e4e480ddf2db44b44a79e14718b5/raw/ded23e586ca5db1b4a566b1e289acd12ebf69357/bikeshare_hourly.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "This run executes all the preprocessing steps with no feature creation or feature selection to get an understaning of model performance in advance of applying more advanced techniques to the data. The models tested are linear regression, random forest, and gradient boosting with their default parameters. We get an $R^{2}$ of 0.8325 on the holdout with our best model which is a Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = [drop_registered, drop_casual, drop_date, drop_instant, year_as_bool,\\\n",
    "      season_as_category, month_as_category, weekday_as_category, \\\n",
    "      hour_as_category, holiday_as_bool, working_day_as_bool, weather_sit_as_category]\n",
    "\n",
    "\n",
    "lm = {\"name\": \"Linear Regression\",\n",
    "      \"model\": LinearRegression(),\n",
    "      \"params\": {}\\\n",
    "     }\n",
    "\n",
    "rfr = {\"name\": \"Random Forest\",\n",
    "       \"model\": RandomForestRegressor(),\n",
    "       \"params\": {\"random_state\": [SEED]}\n",
    "      }\n",
    "\n",
    "gbr = {\"name\": \"Gradient Boosting\",\n",
    "       \"model\": GradientBoostingRegressor(),\n",
    "       \"params\": { \"random_state\": [SEED] }\n",
    "      }\n",
    "\n",
    "models = [lm, rfr, gbr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got a lot of errors below with that line of code related to utils.py\n",
    "Couldn't get get_dummies to work with Dask..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning pipeline at 2019-05-22 14:07:48.598287\n",
      "\n",
      "Performing preprocessing steps...\n",
      "\tDropping the registered variable since we won't have this information\n",
      "\tDropping the causual variable since we won't have this information\n",
      "\tDropping the date variable since this information is encoded in other variables\n",
      "\tDropping index variable\n",
      "\tConverting year to a boolean variable...\n",
      "\tConverting season to a categorical variable...\n",
      "\tConverting month to a categorical variable...\n",
      "\tConverting day of week to a categorical variable...\n",
      "\tConverting hour of day to a categorical variable...\n",
      "\tConverting holiday or not to a boolean variable...\n",
      "\tConverting holiday or not to a boolean variable...\n",
      "\tConverting weather situation to a categorical variable...\n",
      "Preprocessing completed at 2019-05-22 14:07:48.711984, performed 12 steps\n",
      "New Shape of data: 13\n",
      "\n",
      "Performing feature creation...\n",
      "Feature Creation completed at 2019-05-22 14:07:48.715972, performed 0 steps\n",
      "New Shape of data: 13\n",
      "\n",
      "Dummifying...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "`get_dummies` with unknown categories is not supported. Please use `column.cat.as_known()` or `df.categorize()` beforehand to ensure known categories",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f59ed57dcc3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipeline_casero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Downloads\\SUBMIT-20190511T134258Z-001\\DASK Assignment\\utils.py\u001b[0m in \u001b[0;36mpipeline_casero\u001b[1;34m(data, preprocessing, creation, reduction, selection, models)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dummifying...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"New Shape of data: {0}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\reshape.py\u001b[0m in \u001b[0;36mget_dummies\u001b[1;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhas_known_categories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munknown_cat_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;31m# We explicitly create `meta` on `data._meta` (the empty version) to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: `get_dummies` with unknown categories is not supported. Please use `column.cat.as_known()` or `df.categorize()` beforehand to ensure known categories"
     ]
    }
   ],
   "source": [
    "pipeline_casero(data, preprocessing=pp, models=models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Features with Grid Search\n",
    "\n",
    "The next thing we try is adding two basic futures, a boolean field containing whether or not the hour is during commute hours and a field that contains the row's cluster based on clustering using weather variables. To ensure that we're not fitting too much noise, we also apply a feature selection method that simply removes those features that contribute less than 0.1% of the decisions being made by the trees in a simple random forest. Here we also do Grid Search Cross Validation on a small set of parameters for random forest and gradient boosting, namely the number of trees per forest and the learning rate for gradient boosting. Here, we increase our $R^{2}$ on the test set to 0.8625, and our new best model after parameter tuning with grid search is Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = [drop_registered, drop_casual, drop_date, drop_instant, year_as_bool,\\\n",
    "      season_as_category, month_as_category, weekday_as_category, \\\n",
    "      hour_as_category, holiday_as_bool, working_day_as_bool, weather_sit_as_category,\n",
    "     change_season, change_weather_sit]\n",
    "\n",
    "fc = [commute_hours, weather_cluster]\n",
    "\n",
    "fs = [tree_selection]\n",
    "\n",
    "lm = {\"name\": \"Linear Regression\",\n",
    "      \"model\": LinearRegression(),\n",
    "      \"params\": {}\\\n",
    "     }\n",
    "\n",
    "rfr = {\"name\": \"Random Forest\",\n",
    "       \"model\": RandomForestRegressor(),\n",
    "       \"params\": {\"random_state\": [SEED],\n",
    "                  \"n_estimators\": list(range(10, 110, 10))\n",
    "                 }\\\n",
    "      }\n",
    "\n",
    "gbr = {\"name\": \"Gradient Boosting\",\n",
    "       \"model\": GradientBoostingRegressor(),\n",
    "       \"params\": {\"random_state\": [SEED],\n",
    "                  \"n_estimators\": list(range(500, 1050, 50)),\n",
    "                  \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "                 }\\\n",
    "      }\n",
    "\n",
    "models = [lm, rfr, gbr]\n",
    "\n",
    "\n",
    "pipeline_casero(PATH, preprocessing=pp, creation=fc, selection=fs, models=models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Feature Synthesis \n",
    "\n",
    "In the next trial, we added deep features using deep feature synthesis on top of the previous steps we ran on the previous iteration. Unfortunately, our score on the holdout actually decreases to 0.860 as a result of adding this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = [drop_registered, drop_casual, drop_date, drop_instant, year_as_bool,\\\n",
    "      season_as_category, month_as_category, weekday_as_category, \\\n",
    "      hour_as_category, holiday_as_bool, working_day_as_bool, weather_sit_as_category,\n",
    "     change_season, change_weather_sit]\n",
    "\n",
    "\n",
    "fc = [commute_hours, weather_cluster, deep_features]\n",
    "\n",
    "fs = [tree_selection]\n",
    "\n",
    "lm = {\"name\": \"Linear Regression\",\n",
    "      \"model\": LinearRegression(),\n",
    "      \"params\": {}\n",
    "     }\n",
    "\n",
    "rfr = {\"name\": \"Random Forest\",\n",
    "       \"model\": RandomForestRegressor(),\n",
    "       \"params\": {\"random_state\": [SEED],\n",
    "                  \"n_estimators\": list(range(10, 110, 10))\n",
    "                 }\n",
    "      }\n",
    "\n",
    "gbr = {\"name\": \"Gradient Boosting\",\n",
    "       \"model\": GradientBoostingRegressor(),\n",
    "       \"params\": {\"random_state\": [SEED],\n",
    "                  \"n_estimators\": list(range(500, 1050, 50)),\n",
    "                  \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "                 }\n",
    "      }\n",
    "\n",
    "models = [lm, rfr, gbr]\n",
    "\n",
    "\n",
    "pipeline_casero(PATH, preprocessing=pp, creation=fc, selection=fs, models=models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Registered and Casual Predictions\n",
    "\n",
    "The theory behind this approach was that although we don't know the true values of registered and casual numbers of riders when we are trying to make a prediction for the total number of users and therefore cannot use those values directly, what we can do is make predictions about what these values will be and use those as features to a final model. Doing this did not improve our score, probably for a few reasons. First, the cross validation ended up tremendously overfitting because data from \"blind\" sets during cross validation were used to generate the values for those features. This meant that the correlations between these predictions and the target variable were extremely high. One consequence of this is that during feature selection, every feature besides these predictions were ultimately dropped since by far the greatest separating features were these predictions. It also led to Linear Regression being chosen as the ideal model during cross validation because of the small feature space and the overfitted correlations between the features and the target during cross validation. The result is still good because these predictions decently explain the target (for good reason), but the result is not better than previous iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = [drop_registered, drop_casual, drop_date, drop_instant, year_as_bool,\\\n",
    "      season_as_category, month_as_category, weekday_as_category, \\\n",
    "      hour_as_category, holiday_as_bool, working_day_as_bool, weather_sit_as_category,\n",
    "     change_season, change_weather_sit]\n",
    "\n",
    "\n",
    "fc= [commute_hours, weather_cluster, prediction_forecasts]\n",
    "\n",
    "fs = [tree_selection]\n",
    "\n",
    "\n",
    "lm = {\"name\": \"Linear Regression\",\n",
    "      \"model\": LinearRegression(),\n",
    "      \"params\": {}\n",
    "     }\n",
    "\n",
    "rfr = {\"name\": \"Random Forest\",\n",
    "       \"model\": RandomForestRegressor(),\n",
    "       \"params\": {\"random_state\": [SEED],\n",
    "                  \"n_estimators\": list(range(10, 110, 10))\n",
    "                 }\n",
    "      }\n",
    "\n",
    "gbr = {\"name\": \"Gradient Boosting\",\n",
    "       \"model\": GradientBoostingRegressor(),\n",
    "       \"params\": {\"random_state\": [SEED],\n",
    "                  \"n_estimators\": list(range(500, 1050, 50)),\n",
    "                  \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "                 }\n",
    "      }\n",
    "\n",
    "models = [lm, rfr, gbr]\n",
    "\n",
    "\n",
    "pipeline_casero(PATH, preprocessing=pp, creation=fc, selection=fs, models=models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Feature Synthesis with Stepwise\n",
    "\n",
    "\n",
    "This iteration uses the features from the \"Deep Feature Synthesis\" iteration and replaces the simple tree-based feature importance selection with a forward stepwise feature selection where attributes are added that will improve the score until adding the next best feature actually worsens the score. This results in far less features which is better since the model is simpler, but ultimately the lost information hurt the model more than helped it and the score from this model on the holdout is still lower than the best previously seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = [drop_registered, drop_casual, drop_date, drop_instant, year_as_bool,\\\n",
    "      season_as_category, month_as_category, weekday_as_category, \\\n",
    "      hour_as_category, holiday_as_bool, working_day_as_bool, weather_sit_as_category,\n",
    "     change_season, change_weather_sit]\n",
    "\n",
    "\n",
    "fc= [commute_hours, weather_cluster, deep_features]\n",
    "\n",
    "fs = [r2_selection]\n",
    "\n",
    "\n",
    "lm = {\"name\": \"Linear Regression\",\n",
    "      \"model\": LinearRegression(),\n",
    "      \"params\": {}\\\n",
    "     }\n",
    "\n",
    "rfr = {\"name\": \"Random Forest\",\n",
    "       \"model\": RandomForestRegressor(),\n",
    "       \"params\": {\"random_state\": [SEED],\n",
    "                  \"n_estimators\": list(range(10, 110, 10))\n",
    "                 }\\\n",
    "      }\n",
    "\n",
    "gbr = {\"name\": \"Gradient Boosting\",\n",
    "       \"model\": GradientBoostingRegressor(),\n",
    "       \"params\": {\"random_state\": [SEED],\n",
    "                  \"n_estimators\": list(range(500, 1050, 50)),\n",
    "                  \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "                 }\\\n",
    "      }\n",
    "\n",
    "models = [lm, rfr, gbr]\n",
    "\n",
    "\n",
    "pipeline_casero(PATH, preprocessing=pp, creation=fc, selection=fs, models=models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registered and Casual Forecast with Stepwise\n",
    "\n",
    "In this trial we use registered and casual forecast like before, but this time we update the feature selection method to see if there are other variables that when added to the data will still improve the $R^{2}$. Certain features were still added after the forecast variables, but their contribution was very small and this model also suffers from overfitting, so our overall score on the holdout still does not improve from our best model so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = [drop_registered, drop_casual, drop_date, drop_instant, year_as_bool,\\\n",
    "      season_as_category, month_as_category, weekday_as_category, \\\n",
    "      hour_as_category, holiday_as_bool, working_day_as_bool, weather_sit_as_category,\n",
    "     change_season, change_weather_sit]\n",
    "\n",
    "\n",
    "fc= [commute_hours, weather_cluster, prediction_forecasts, deep_features]\n",
    "\n",
    "fs = [r2_selection]\n",
    "\n",
    "\n",
    "lm = {\"name\": \"Linear Regression\",\n",
    "      \"model\": LinearRegression(),\n",
    "      \"params\": {}\n",
    "     }\n",
    "\n",
    "rfr = {\"name\": \"Random Forest\",\n",
    "       \"model\": RandomForestRegressor(),\n",
    "       \"params\": {\"random_state\": [SEED],\n",
    "                  \"n_estimators\": list(range(10, 110, 10))\n",
    "                 }\n",
    "      }\n",
    "\n",
    "gbr = {\"name\": \"Gradient Boosting\",\n",
    "       \"model\": GradientBoostingRegressor(),\n",
    "       \"params\": {\"random_state\": [SEED],\n",
    "                  \"n_estimators\": list(range(500, 1050, 50)),\n",
    "                  \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "                 }\n",
    "      }\n",
    "\n",
    "models = [lm, rfr, gbr]\n",
    "\n",
    "\n",
    "pipeline_casero(PATH, preprocessing=pp, creation=fc, selection=fs, models=models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Registered and Casual Forecasts with PCA\n",
    "\n",
    "Here we try to do principle component analysis rather than feature selection to see if implicitly including all the other variables into the forecast variables via linear transformation will improve our result. The answer is no - this was a disaster, resulting in a negative $R^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = [drop_registered, drop_casual, drop_date, drop_instant, year_as_bool,\\\n",
    "      season_as_category, month_as_category, weekday_as_category, \\\n",
    "      hour_as_category, holiday_as_bool, working_day_as_bool, weather_sit_as_category,\n",
    "     change_season, change_weather_sit]\n",
    "\n",
    "\n",
    "fc= [commute_hours, weather_cluster, prediction_forecasts, deep_features]\n",
    "\n",
    "dr = [pca]\n",
    "\n",
    "\n",
    "lm = {\"name\": \"Linear Regression\",\n",
    "      \"model\": LinearRegression(),\n",
    "      \"params\": {}\n",
    "     }\n",
    "\n",
    "rfr = {\"name\": \"Random Forest\",\n",
    "       \"model\": RandomForestRegressor(),\n",
    "       \"params\": {\"random_state\": [SEED],\n",
    "                  \"n_estimators\": list(range(10, 110, 10))\n",
    "                 }\n",
    "      }\n",
    "\n",
    "gbr = {\"name\": \"Gradient Boosting\",\n",
    "       \"model\": GradientBoostingRegressor(),\n",
    "       \"params\": {\"random_state\": [SEED],\n",
    "                  \"n_estimators\": list(range(500, 1050, 50)),\n",
    "                  \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "                 }\n",
    "      }\n",
    "\n",
    "models = [lm, rfr, gbr]\n",
    "\n",
    "\n",
    "pipeline_casero(PATH, preprocessing=pp, creation=fc, reduction=dr, models=models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Registered and Casual only with Tree-Based Models\n",
    "\n",
    "Since we figured that linear regression was only being chosen with the forecast variables due to overfitting, we wanted to see if excluding that model from consideration would help our cause. Without a linear regression model, gradient boosting was selected again as the best model but did not perform better than the linear model trained with these variables and therefore still worse than our best score so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = [drop_registered, drop_casual, drop_date, drop_instant, year_as_bool,\\\n",
    "      season_as_category, month_as_category, weekday_as_category, \\\n",
    "      hour_as_category, holiday_as_bool, working_day_as_bool, weather_sit_as_category,\n",
    "     change_season, change_weather_sit]\n",
    "\n",
    "\n",
    "fc= [commute_hours, weather_cluster, prediction_forecasts]\n",
    "\n",
    "fs = [tree_selection]\n",
    "\n",
    "\n",
    "lm = {\"name\": \"Linear Regression\",\n",
    "      \"model\": LinearRegression(),\n",
    "      \"params\": {}\n",
    "     }\n",
    "\n",
    "rfr = {\"name\": \"Random Forest\",\n",
    "       \"model\": RandomForestRegressor(),\n",
    "       \"params\": {\"random_state\": [SEED],\n",
    "                  \"n_estimators\": list(range(10, 110, 10))\n",
    "                 }\n",
    "      }\n",
    "\n",
    "gbr = {\"name\": \"Gradient Boosting\",\n",
    "       \"model\": GradientBoostingRegressor(),\n",
    "       \"params\": {\"random_state\": [SEED],\n",
    "                  \"n_estimators\": list(range(500, 1050, 50)),\n",
    "                  \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "                 }\n",
    "      }\n",
    "\n",
    "models = [rfr, gbr]\n",
    "\n",
    "\n",
    "pipeline_casero(PATH, preprocessing=pp, creation=fc, selection=fs, models=models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registered/Casual, Deep Features, Stepwise, Only Trees\n",
    "\n",
    "Here we use forecasted variables along with deep features and stepwise feature selection, again only with decision trees, to see what the result will be from this combination. There is still no improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = [drop_registered, drop_casual, drop_date, drop_instant, year_as_bool,\\\n",
    "      season_as_category, month_as_category, weekday_as_category, \\\n",
    "      hour_as_category, holiday_as_bool, working_day_as_bool, weather_sit_as_category,\n",
    "     change_season, change_weather_sit]\n",
    "\n",
    "\n",
    "fc= [commute_hours, weather_cluster, prediction_forecasts, deep_features]\n",
    "\n",
    "fs = [r2_selection]\n",
    "\n",
    "\n",
    "lm = {\"name\": \"Linear Regression\",\n",
    "      \"model\": LinearRegression(),\n",
    "      \"params\": {}\n",
    "     }\n",
    "\n",
    "rfr = {\"name\": \"Random Forest\",\n",
    "       \"model\": RandomForestRegressor(),\n",
    "       \"params\": {\"random_state\": [SEED],\n",
    "                  \"n_estimators\": list(range(10, 110, 10))\n",
    "                 }\n",
    "      }\n",
    "\n",
    "gbr = {\"name\": \"Gradient Boosting\",\n",
    "       \"model\": GradientBoostingRegressor(),\n",
    "       \"params\": {\"random_state\": [SEED],\n",
    "                  \"n_estimators\": list(range(500, 1050, 50)),\n",
    "                  \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "                 }\n",
    "      }\n",
    "\n",
    "models = [rfr, gbr]\n",
    "\n",
    "\n",
    "pipeline_casero(PATH, preprocessing=pp, creation=fc, selection=fs, models=models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination w/ Deep Features\n",
    "\n",
    "This iteration is our best model in terms of performance on the holdout.\n",
    "\n",
    "This is a variation of the \"Deep Feature Synthesis\" trial, except this time instead of removing several features at once based on feature importance in a random forest, we only remove them one at a time and refit recursively until the number of features is reduced to 50. This means less features will be used but the features have been taken away more intelligently, and since we are using deep feature synthesis we have created a large number of features to choose from in terms of explaining the target variable.\n",
    "\n",
    "The final $R^{2}$ on the holdout is 0.8638.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = [drop_registered, drop_casual, drop_date, drop_instant, year_as_bool,\\\n",
    "      season_as_category, month_as_category, weekday_as_category, \\\n",
    "      hour_as_category, holiday_as_bool, working_day_as_bool, weather_sit_as_category,\n",
    "     change_season, change_weather_sit]\n",
    "\n",
    "fc= [commute_hours, weather_cluster, deep_features]\n",
    "\n",
    "fs = [rfe]\n",
    "\n",
    "models=[gbr]\n",
    "\n",
    "pipeline_casero(PATH, preprocessing=pp, creation=fc, selection=fs, models=models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination with Basic Features\n",
    "\n",
    "\n",
    "Since previously our best iteration had been basic features without deep feature synthesis, we wanted to try recursive feature elimination on just those features to see if we would improve our score from the previous iteration. We see that we actually did not do better, which demonstrate that some of the values produced from deep feature synthesis actually did turn out to be an important deciding variable for predicting the number of riders in an hour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = [drop_registered, drop_casual, drop_date, drop_instant, year_as_bool,\\\n",
    "      season_as_category, month_as_category, weekday_as_category, \\\n",
    "      hour_as_category, holiday_as_bool, working_day_as_bool, weather_sit_as_category,\n",
    "     change_season, change_weather_sit]\n",
    "\n",
    "fc= [commute_hours, weather_cluster]\n",
    "\n",
    "fs = [rfe]\n",
    "\n",
    "models=[gbr]\n",
    "\n",
    "pipeline_casero(PATH, preprocessing=pp, creation=fc, selection=fs, models=models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
